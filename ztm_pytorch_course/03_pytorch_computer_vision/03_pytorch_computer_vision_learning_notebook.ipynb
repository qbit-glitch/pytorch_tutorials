{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d8e30cd",
   "metadata": {},
   "source": [
    "# 03. Pytorch Computer Vision\n",
    "\n",
    "![Pytorch Computer Vision Workflow](../assets/03-pytorch-computer-vision-workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea02c8cf",
   "metadata": {},
   "source": [
    "## 0. Computer Vision Libraries in Pytorch\n",
    "\n",
    "| **PyTorch module** | **What does it do?** |\n",
    "| --- | --- |\n",
    "| [`torchvision`](https://pytorch.org/vision/stable/index.html) | Contains datasets, model architectures and image transformations often used for computer vision problems. |\n",
    "| [`torchvision.datasets`](https://pytorch.org/vision/stable/datasets.html) | Here you'll find many example computer vision datasets for a range of problems from image classification, object detection, image captioning, video classification and more. It also contains [a series of base classes for making custom datasets](https://pytorch.org/vision/stable/datasets.html#base-classes-for-custom-datasets). |\n",
    "| [`torchvision.models`](https://pytorch.org/vision/stable/models.html) | This module contains well-performing and commonly used computer vision model architectures implemented in PyTorch, you can use these with your own problems. |\n",
    "| [`torchvision.transforms`](https://pytorch.org/vision/stable/transforms.html) | Often images need to be transformed (turned into numbers/processed/augmented) before being used with a model, common image transformations are found here. |\n",
    "| [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) | Base dataset class for PyTorch. |\n",
    "| [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#module-torch.utils.data) | Creates a Python iterable over a dataset (created with `torch.utils.data.Dataset`). |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a53877f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 2.8.0\n",
      "torchvision version: 0.23.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import  matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"Pytorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901a4a49",
   "metadata": {},
   "source": [
    "## 1. Getting a Dataset\n",
    "\n",
    "![Fashion MNIST Dataset](../assets/03-fashion-mnist-slide.png)\n",
    "\n",
    "- `torchvision.datasets` contains a lot of example datasets you can use to practice writing computer vision code on. FashionMNIST is one of those datasets. And since it has 10 different image classes (different types of clothing), it's a multi-class classification problem.\n",
    "\n",
    "- PyTorch has a bunch of computer vision datasets stored in torchvision.datasets. Including FashionMNIST in `torchvision.datasets.FashionMNIST()`\n",
    "\n",
    "\n",
    "To download it, we provide the following parameters:\n",
    "\n",
    "- root: str - which folder do you want to download the data to?\n",
    "\n",
    "- train: Bool - do you want the training or test split?\n",
    "\n",
    "- download: Bool - should the data be downloaded?\n",
    "\n",
    "- transform: torchvision.transforms - what transformations would you like to do on the data?\n",
    "\n",
    "- target_transform - you can transform the targets (labels) if you like too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8405c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training data\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root = 'data',\n",
    "    train = True,   # get training data\n",
    "    download = True,    \n",
    "    transform = ToTensor(),     # Images come as a PIL format, we want to turn into Torch tensors\n",
    "    target_transform = None,\n",
    ")\n",
    "\n",
    "# Setup testing data\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = 'data',\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06688e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
       "           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0039, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
       "           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
       "           0.0157, 0.0000, 0.0000, 0.0118],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
       "           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0471, 0.0392, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
       "           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
       "           0.3020, 0.5098, 0.2824, 0.0588],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
       "           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
       "           0.5529, 0.3451, 0.6745, 0.2588],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
       "           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
       "           0.4824, 0.7686, 0.8980, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
       "           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
       "           0.8745, 0.9608, 0.6784, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
       "           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
       "           0.8627, 0.9529, 0.7922, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
       "           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
       "           0.8863, 0.7725, 0.8196, 0.2039],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
       "           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
       "           0.9608, 0.4667, 0.6549, 0.2196],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
       "           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
       "           0.8510, 0.8196, 0.3608, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
       "           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
       "           0.8549, 1.0000, 0.3020, 0.0000],\n",
       "          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
       "           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
       "           0.8784, 0.9569, 0.6235, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
       "           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
       "           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
       "           0.9137, 0.9333, 0.8431, 0.0000],\n",
       "          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
       "           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
       "           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
       "           0.8627, 0.9098, 0.9647, 0.0000],\n",
       "          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
       "           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
       "           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
       "           0.8706, 0.8941, 0.8824, 0.0000],\n",
       "          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
       "           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
       "           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
       "           0.8745, 0.8784, 0.8980, 0.1137],\n",
       "          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
       "           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
       "           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
       "           0.8627, 0.8667, 0.9020, 0.2627],\n",
       "          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
       "           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
       "           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
       "           0.7098, 0.8039, 0.8078, 0.4510],\n",
       "          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
       "           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
       "           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
       "           0.6549, 0.6941, 0.8235, 0.3608],\n",
       "          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
       "           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
       "           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
       "           0.7529, 0.8471, 0.6667, 0.0000],\n",
       "          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
       "           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
       "           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
       "           0.3882, 0.2275, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
       "           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See first training sample\n",
    "image, label = train_data[0]\n",
    "image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9894775",
   "metadata": {},
   "source": [
    "### 1.1 Input and Output shape of a Computer Vision model\n",
    "\n",
    "- We've got a big tensor of values (the image) leading to a single value for the target (the label).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22fc6e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's the shape of the image ?\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8be21ae",
   "metadata": {},
   "source": [
    "`[color_channels=1, height=28, width=28]`\n",
    "\n",
    "- `color_channels = 1` means the image is greyscale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4c812b",
   "metadata": {},
   "source": [
    "![Input and Output Shapes](../assets/03-computer-vision-input-and-output-shapes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9030a4c0",
   "metadata": {},
   "source": [
    "- Various problems will have various input and output shapes. But the premise remains: encode data into numbers, build a model to find patterns in those numbers, convert those patterns into something meaningful.\n",
    "\n",
    "- If color_channels=3, the image comes in pixel values for red, green and blue (this is also known as the RGB color model).\n",
    "\n",
    "- The order of our current tensor is often referred to as `CHW` (Color Channels, Height, Width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bd5692b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 60000, 10000, 10000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check how many samples are there\n",
    "len(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "956d5c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see classes\n",
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3935716b",
   "metadata": {},
   "source": [
    "### 1.2 Visualizing our Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3475062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape: torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '9')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image, label = train_data[0]\n",
    "print(f\"Image Shape: {image.shape}\")\n",
    "plt.imshow(image.squeeze())     # image shape is [1,28,28]  (color_channels, height, width)\n",
    "plt.title(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f0923f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Ankle boot')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "plt.title(class_names[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77585c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x900 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot more images\n",
    "torch.manual_seed(42)\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "rows, cols = 4,4\n",
    "for i in range(1, rows*cols+1):\n",
    "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
    "    img, label = train_data[random_idx]\n",
    "    fig.add_subplot(rows, cols, i)\n",
    "    plt.imshow(img.squeeze(), cmap='gray')\n",
    "    plt.title(class_names[label])\n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbe10b8",
   "metadata": {},
   "source": [
    "## 2. Prepare DataLoader\n",
    "\n",
    "DataLoader - It helps load data into a model. For training and for inference. It turns a large Dataset into Python iterable of smaller chunks. These smaller chunks are called batches or mini-batches and can be set by the batch_size parameter.\n",
    "\n",
    "It's more computationally efficient.\n",
    "\n",
    "- With mini-batches (small portions of the data), gradient descent is performed more often per epoch (once per mini-batch rather than once per epoch).\n",
    "\n",
    "- Good Batch Size: 32 is a good place to start. But since this is a value you can set (a hyperparameter) you can try all different kinds of values, though generally powers of 2 are used most often (e.g. 32, 64, 128, 256, 512).\n",
    "\n",
    "Let's create a DataLoader for our training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a52e516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader: (<torch.utils.data.dataloader.DataLoader object at 0x1544ce080>, <torch.utils.data.dataloader.DataLoader object at 0x1544cfa60>)\n",
      "Length of train dataloader: 1875 batches of 32\n",
      "Length of test dataloader: 313 batches of 32\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Turn datasets into iterables (batches)\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Let's check out what we've created\n",
    "print(f\"Dataloader: {train_dataloader, test_dataloader}\")\n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b05da9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 28, 28]), torch.Size([32]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out what's insise the training dataloader\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "train_features_batch.shape, train_labels_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46d02c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: torch.Size([1, 28, 28])\n",
      "Label: 6, label size: torch.Size([])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show a sample\n",
    "torch.manual_seed(42)\n",
    "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
    "img, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
    "plt.imshow(img.squeeze(), cmap='gray')\n",
    "plt.title(class_names[label])\n",
    "plt.axis('Off')\n",
    "print(f\"Image size: {img.shape}\")\n",
    "print(f\"Label: {label}, label size: {label.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f952314",
   "metadata": {},
   "source": [
    "## 3. Model 0: Build a baseline model\n",
    "\n",
    "Time to build a baseline model by subclassing nn.Module.\n",
    "\n",
    "A baseline model is one of the simplest models you can imagine.\n",
    "\n",
    "You use the baseline as a starting point and try to improve upon it with subsequent, more complicated models.\n",
    "\n",
    "Because we're working with image data, we're going to use a different layer to start things off.\n",
    "\n",
    "And that's the nn.Flatten() layer.\n",
    "\n",
    "nn.Flatten() compresses the dimensions of a tensor into a single vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a42f984f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before flattening: torch.Size([1, 28, 28]) -> [c, h, w]\n",
      "Shape after flattening: torch.Size([1, 784]) -> [c, h*w]\n"
     ]
    }
   ],
   "source": [
    "# Create a flatten layer\n",
    "flatten_model = nn.Flatten()    # all nn modules functions as a model (can do a forward pass)\n",
    "# Get a single sample\n",
    "x = train_features_batch[0]\n",
    "\n",
    "# Flatten the sample\n",
    "output = flatten_model(x)   # perform forward pass\n",
    "\n",
    "# Print out what happened\n",
    "print(f\"Shape before flattening: {x.shape} -> [c, h, w]\")\n",
    "print(f\"Shape after flattening: {output.shape} -> [c, h*w]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f607ce74",
   "metadata": {},
   "source": [
    "The nn.Flatten() layer took our shape from [color_channels, height, width] to [color_channels, height*width].\n",
    "\n",
    "Why do this?\n",
    "\n",
    "Because we've now turned our pixel data from height and width dimensions into one long feature vector.\n",
    "\n",
    "And nn.Linear() layers like their inputs to be in the form of feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26c34e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class FashionMNISTModelV0(nn.Module):\n",
    "    def __init__(self, input_shape:int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features = input_shape, out_features = hidden_units),\n",
    "            nn.Linear(in_features = hidden_units, out_features = output_shape)            \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c510409f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModelV0(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Need to setup model with input paramters\n",
    "model_0 = FashionMNISTModelV0(input_shape = 784, hidden_units = 10, output_shape = len(class_names))\n",
    "\n",
    "model_0.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa1e0c2",
   "metadata": {},
   "source": [
    "### 3.1 Setup Loss, Optimizer and evaluation metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed2b971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the accuracy function\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = correct / len(y_pred) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0796cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()     # this is also called criterion or cost function\n",
    "optimizer = torch.optim.SGD(params = model_0.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3ba56f",
   "metadata": {},
   "source": [
    "### 3.2 Crearting a function to time our experiments\n",
    "\n",
    "let's make a timing function to measure the time it takes our model to train on CPU versus using a GPU.\n",
    "\n",
    "We'll train this model on the CPU but the next one on the GPU and see what happens.\n",
    "\n",
    "Our timing function will import the timeit.default_timer() function from the Python timeit module.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cabd6a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    \"\"\" Prints difference between start and end time\n",
    "    Args:\n",
    "        start (float): start time of computation (preferred in timeit format)\n",
    "        end (float) : End time of computation\n",
    "        device ([type], optional) : Device that compute is running on. Defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "        float: time between start and end in seconds (higher is longer)\n",
    "    \"\"\"\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9867d90f",
   "metadata": {},
   "source": [
    "### 3.3 Creating a training loop and training a model on a batches of data\n",
    "\n",
    "- we will add another loop to loop through our data batches\n",
    "\n",
    "- Our databatches are contained within our Dataloader's train_dataloader, test_dataloader for the training and test data splits respectively.\n",
    "\n",
    "- Since we are computing on batches of data, out loss functions and evaluation metrics will be calculated per batch rather than accross the whole dataset.\n",
    "\n",
    "- this means, we'll have to divide our loss and accuracy values by the number of batches in each dataset's respective dataloader.\n",
    "\n",
    "- Let's step through it : \n",
    "    1. Loop through epochs\n",
    "    2. Loop through training batches. perform training steps, calculate the train loss per batch\n",
    "    3. Loop through testing batches, perform testing steps, calculate the test loss per batch\n",
    "    4. Print out what's happening\n",
    "    5. time it all (for fun)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "388a7199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f76e0666804faebc6dac633b43a3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "-----------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.59039 | Test loss: 0.50954, Test acc: 82.04%\n",
      "\n",
      "Epoch: 1\n",
      "-----------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.47633 | Test loss: 0.47989, Test acc: 83.20%\n",
      "\n",
      "Epoch: 2\n",
      "-----------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.45503 | Test loss: 0.47664, Test acc: 83.43%\n",
      "\n",
      "Train time on cpu: 5.732 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# set the seed and start the timer\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_cpu = timer()\n",
    "\n",
    "# Set the number of epochs (we'll keep this small for faster training times)\n",
    "epochs = 3\n",
    "# device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# creating training and testing loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n-----------\")\n",
    "    ### training\n",
    "    train_loss = 0\n",
    "    # Add a loop to loop through training batches\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model_0.train()\n",
    "        # 1. Forward pass\n",
    "        y_pred = model_0(X)\n",
    "        \n",
    "        # 2. Calculate loss (per batch)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "        \n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print out how many samples have been seen\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "            \n",
    "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    \n",
    "    ### Testing\n",
    "    # Setup variables for accumulatively adding up loss and accuracy\n",
    "    test_loss, test_acc = 0,0\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X,y in test_dataloader:\n",
    "            # Forward pass\n",
    "            test_pred = model_0(X)\n",
    "            \n",
    "            # Calculate the loss (accumulatively)\n",
    "            test_loss += loss_fn(test_pred, y)      # accumulatively add up the loss per epoch \n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "            \n",
    "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
    "        # divide total test loss by length of test dataloader (per batch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "        \n",
    "        # Divide total accuracy by length of test dataloader (per batch)\n",
    "        test_acc /= len(test_dataloader)\n",
    "        \n",
    "    # Print out what's happending\n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
    "    \n",
    "\n",
    "# Calculate training time\n",
    "train_time_end_on_cpu = timer()\n",
    "total_train_time_model_0 = print_train_time(\n",
    "    start = train_time_start_on_cpu,\n",
    "    end = train_time_end_on_cpu,\n",
    "    device = str(next(model_0.parameters()).device)\n",
    ")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a256af44",
   "metadata": {},
   "source": [
    "## 4. Make predictions and get Model 0 results\n",
    "\n",
    "- Namely, let's create a function that takes in a trained model, a DataLoader, a loss function and an accuracy function.\n",
    "\n",
    "- The function will use the model to make predictions on the data in the DataLoader and then we can evaluate those predictions using the loss function and accuracy function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebbe2c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'FashionMNISTModelV0',\n",
       " 'model_loss': 0.47663894295692444,\n",
       " 'model_acc': 83.42651757188499}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, accuracy_fn):\n",
    "    \"\"\" Returns a dictionary containing the results of model predicting on data_loader\n",
    "    Args:\n",
    "        - model (torch.nn.Module): A pytorch model capable of making predictions on data_lodaer\n",
    "        - data_loader (torch.utils.data.DataLoader) : The target dataset to predict on\n",
    "        - loss_fn (torch.nn.Module) : The loss function of model\n",
    "        - accuracy_fn : An accuracy function to compare the models predictions to the truth labels\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader\n",
    "    \"\"\"\n",
    "    loss, acc = 0,0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X,y in data_loader:\n",
    "            # make predictions with the model\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # Accumulate the loss and accuracy values per batch\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true = y, y_pred = y_pred.argmax(dim=1))\n",
    "            \n",
    "        # Scale loss and accuracy to find the average loss/acc per \n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "        \n",
    "    return {\n",
    "        \"model_name\": model.__class__.__name__,\n",
    "        'model_loss': loss.item(),\n",
    "        'model_acc': acc,\n",
    "    }\n",
    "    \n",
    "# Calculate model 0 results on test dataset\n",
    "model_0_results = eval_model(model = model_0, data_loader = test_dataloader, loss_fn = loss_fn, accuracy_fn = accuracy_fn)\n",
    "\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc66c029",
   "metadata": {},
   "source": [
    "### 5. Setup device agnostic code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e027b2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f297b3",
   "metadata": {},
   "source": [
    "## 6. Model-1: Building a better model with non-linearity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c32e22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps:0\n",
      "FashionMNISTModelV1(\n",
      "  (layer_stack): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create a model with non-linear and linear layers\n",
    "class FashionMNISTModelV1(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units:int, output_shape:int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.layer_stack(x)\n",
    "    \n",
    "    \n",
    "torch.manual_seed(42)\n",
    "model_1 = FashionMNISTModelV1(input_shape = 784, hidden_units = 10, output_shape = len(class_names)).to(device)\n",
    "\n",
    "print(next(model_1.parameters()).device)\n",
    "print(model_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c456b5c",
   "metadata": {},
   "source": [
    "### 6.1 Setup Loss, Optimizer and evaluation metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3ca2346",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params = model_1.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cff8e51",
   "metadata": {},
   "source": [
    "### 6.2 Functionizing Training and Test Loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea4ea661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, \n",
    "                data_loader: torch.utils.data.DataLoader,\n",
    "                loss_fn: torch.nn.Module,\n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                accuracy_fn,\n",
    "                device: torch.device = device):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    model.to(device)\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        # Send data to GPU\n",
    "        X,y = X.to(device), y.to(device)\n",
    "        # 1. Forward Pass\n",
    "        y_pred = model(X)\n",
    "        \n",
    "        # 2. Calculate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "        train_acc += accuracy_fn(y_true = y,\n",
    "                                 y_pred = y_pred.argmax(dim=1))\n",
    "        \n",
    "        # Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Loss backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Calculate loss and accuracy per epoch and print out what's happenning\n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "674e5739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(data_loader: torch.utils.data.DataLoader,\n",
    "              model: torch.nn.Module, \n",
    "              loss_fn: torch.nn.Module,\n",
    "              accuracy_fn,\n",
    "              device: torch.device = device):\n",
    "    test_loss, test_acc = 0,0\n",
    "    model.to(device)\n",
    "    model.eval()    # put model in eval mode\n",
    "    # Turn on inference contect manager\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Send data to GPU\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # forward pass\n",
    "            test_pred = model(X)\n",
    "            \n",
    "            # Calculate loss and accuracy\n",
    "            test_loss += loss_fn(test_pred, y)\n",
    "            test_acc += accuracy_fn(y_true=y,\n",
    "                                    y_pred = test_pred.argmax(dim=1))\n",
    "        \n",
    "        # Adjust metrics and print out\n",
    "        test_loss /= len(data_loader)\n",
    "        test_acc /= len(data_loader)\n",
    "        print(f\"Test Loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4440afc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7d12ae785147db9c7ab18da6158cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "----------------\n",
      "Train loss: 1.09199 | Train accuracy: 61.34\n",
      "Test Loss: 0.95636 | Test accuracy: 65.00%\n",
      "\n",
      "Epoch: 1\n",
      "----------------\n",
      "Train loss: 0.78097 | Train accuracy: 71.94\n",
      "Test Loss: 0.72611 | Test accuracy: 73.99%\n",
      "\n",
      "Epoch: 2\n",
      "----------------\n",
      "Train loss: 0.67040 | Train accuracy: 75.94\n",
      "Test Loss: 0.69007 | Test accuracy: 74.81%\n",
      "\n",
      "Train time on mps: 11.091 seconds\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# measure time\n",
    "from timeit import default_timer as timer\n",
    "train_time_start_on_gpu = timer()\n",
    "\n",
    "epochs = 3\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n----------------\")\n",
    "    train_step(data_loader = train_dataloader,\n",
    "               model = model_1,\n",
    "               loss_fn = loss_fn,\n",
    "               optimizer = optimizer,\n",
    "               accuracy_fn = accuracy_fn)\n",
    "    test_step(data_loader = test_dataloader,\n",
    "              model = model_1,\n",
    "              loss_fn = loss_fn,\n",
    "              accuracy_fn = accuracy_fn)\n",
    "    \n",
    "train_time_end_on_gpu = timer()\n",
    "total_train_time_model_1 = print_train_time(start = train_time_start_on_gpu,\n",
    "                                            end = train_time_end_on_gpu,\n",
    "                                            device = device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7507f3",
   "metadata": {},
   "source": [
    "Our model trained but the training time took longer?\n",
    "\n",
    "Note: The training time on CUDA vs CPU will depend largely on the quality of the CPU/GPU you're using. Read on for a more explained answer.\n",
    "\n",
    "Question: \"I used a GPU but my model didn't train faster, why might that be?\"\n",
    "\n",
    "Answer: Well, one reason could be because your dataset and model are both so small (like the dataset and model we're working with) the benefits of using a GPU are outweighed by the time it actually takes to transfer the data there.\n",
    "\n",
    "There's a small bottleneck between copying data from the CPU memory (default) to the GPU memory.\n",
    "\n",
    "So for smaller models and datasets, the CPU might actually be the optimal place to compute on.\n",
    "\n",
    "But for larger datasets and models, the speed of computing the GPU can offer usually far outweighs the cost of getting the data there.\n",
    "\n",
    "However, this is largely dependent on the hardware you're using. With practice, you will get used to where the best place to train your models is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "949603ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this code to see the errors\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "# # note: this will error due to `eval_model()` not using device agnostic code\n",
    "# model_1_results = eval_model(model = model_1,\n",
    "#                              data_loader = test_dataloader,\n",
    "#                              loss_fn = loss_fn,\n",
    "#                              accuracy_fn = accuracy_fn)\n",
    "# model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69f6c2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'FashionMNISTModelV1',\n",
       " 'model_loss': 0.6900655031204224,\n",
       " 'model_acc': 74.810303514377}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move values to device\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def eval_model(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device):\n",
    "    \"\"\"Evaluates a given model on a given dataset\n",
    "    \n",
    "    Args:\n",
    "        - model (torch.nn.Module) : A Pytorch model capable of making predictions on data_loader\n",
    "        - data_loader (torch.utils.data.DataLoader) : The target dataset to predict on\n",
    "        - loss_fn (torch.nn.Module) : The loss function of model\n",
    "        - accuracy_fn : An accuracy function to compare the models predictions to the truth labels\n",
    "        - device (str, optional): Target device to compute on. Defaults to device\n",
    "    \n",
    "    Returns:\n",
    "        - (dict) : Results of model making predictions on data_loader\n",
    "    \"\"\"\n",
    "    loss, acc = 0,0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X,y in data_loader:\n",
    "            # Send data to the target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model(X)\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
    "            \n",
    "        # Scale loss and acc\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "        \n",
    "    return {\n",
    "        \"model_name\": model.__class__.__name__,\n",
    "        'model_loss': loss.item(),\n",
    "        'model_acc': acc\n",
    "    }\n",
    "    \n",
    "# Calculate model 1 results with device agnostic code\n",
    "model_1_results = eval_model(model = model_1,\n",
    "                             data_loader = test_dataloader,\n",
    "                             loss_fn = loss_fn,\n",
    "                             accuracy_fn = accuracy_fn,\n",
    "                             device = device)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04991a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'FashionMNISTModelV0',\n",
       " 'model_loss': 0.47663894295692444,\n",
       " 'model_acc': 83.42651757188499}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check baseline results\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114ee6a",
   "metadata": {},
   "source": [
    "Woah, in this case, it looks like adding non-linearities to our model made it perform worse than the baseline.\n",
    "\n",
    "That's a thing to note in machine learning, sometimes the thing you thought should work doesn't.\n",
    "\n",
    "And then the thing you thought might not work does.\n",
    "\n",
    "It's part science, part art.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca51772",
   "metadata": {},
   "source": [
    "From the looks of things, it seems like our model is overfitting on the training data.\n",
    "\n",
    "Overfitting means our model is learning the training data well but those patterns aren't generalizing to the testing data.\n",
    "\n",
    "Two of the main ways to fix overfitting include:\n",
    "\n",
    "- Using a smaller or different model (some models fit certain kinds of data better than others).\n",
    "\n",
    "- Using a larger dataset (the more data, the more chance a model has to learn generalizable patterns)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f4e902",
   "metadata": {},
   "source": [
    "There are more, but I'm going to leave that as a challenge for you to explore.\n",
    "\n",
    "Try searching online, \"ways to prevent overfitting in machine learning\" and see what comes up.\n",
    "\n",
    "In the meantime, let's take a look at number 1: using a different model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbcc5c6",
   "metadata": {},
   "source": [
    "## 7. Model 2: Building a Convolutional Neural Network (CNN)\n",
    "\n",
    "[CNN Explainer Website](https://poloclub.github.io/cnn-explainer/)\n",
    "\n",
    "CNN's are known for their capabilities to find patterns in visual data.\n",
    "\n",
    "And since we're dealing with visual data, let's see if using a CNN model can improve upon our baseline.\n",
    "\n",
    "The CNN model we're going to be using is known as TinyVGG from the CNN Explainer website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29777298",
   "metadata": {},
   "source": [
    "It follows the typical structure of a convolutional neural network:\n",
    "\n",
    "`Input layer -> [Convolutional layer -> activation layer -> pooling layer] -> Output layer`\n",
    "\n",
    "Where the contents of `[Convolutional layer -> activation layer -> pooling layer]` can be upscaled and repeated multiple times, depending on requirements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3115626c",
   "metadata": {},
   "source": [
    "#### What model should I use\n",
    "\n",
    "| **Problem type** | **Model to use (generally)** | **Code example** |\n",
    "| --- | --- | --- |\n",
    "| Structured data (Excel spreadsheets, row and column data) | Gradient boosted models, Random Forests, XGBoost | [`sklearn.ensemble`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble), [XGBoost library](https://xgboost.readthedocs.io/en/stable/) |\n",
    "| Unstructured data (images, audio, language) | Convolutional Neural Networks, Transformers | [`torchvision.models`](https://pytorch.org/vision/stable/models.html), [HuggingFace Transformers](https://huggingface.co/docs/transformers/index) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f6ba86",
   "metadata": {},
   "source": [
    "Note: The table above is only for reference, the model you end up using will be highly dependent on the problem you're working on and the constraints you have (amount of data, latency requirements).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5848a444",
   "metadata": {},
   "source": [
    "To do so, we'll leverage the nn.Conv2d() and nn.MaxPool2d() layers from torch.nn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7311c066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModelV2(\n",
       "  (block_1): Sequential(\n",
       "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block_2): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=490, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a convolutional neural network\n",
    "class FashionMNISTModelV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Model architecture copying TinyVGG from: \n",
    "    https://poloclub.github.io/cnn-explainer/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape:int, hidden_units:int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = input_shape, \n",
    "                      out_channels = hidden_units,\n",
    "                      kernel_size = 3, # how big is the square that's going over the image ?\n",
    "                      stride = 1,   # default\n",
    "                      padding = 1), # options = 'valid' (no padding) or `same` (output has the same shape as input) or int for specified number\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = hidden_units,\n",
    "                      out_channels = hidden_units,\n",
    "                      kernel_size = 1,\n",
    "                      stride = 1,\n",
    "                      padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2,\n",
    "                         stride = 1)    # default stride value is same as the kernel size\n",
    "        )        \n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # Where did this in_features shape come from ?\n",
    "            # It's because each layer of our network compresses and changes the shape of our input data\n",
    "            nn.Linear(in_features = hidden_units * 7 * 7,\n",
    "                      out_features = output_shape)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.block_1(x)\n",
    "        x = self.block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "torch.manual_seed(42)\n",
    "model_2 = FashionMNISTModelV2(input_shape = 1, hidden_units = 10, \n",
    "                              output_shape = len(class_names)\n",
    "                              ).to(device)\n",
    "model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0850c9dc",
   "metadata": {},
   "source": [
    "What we have done is a common practice in Machine Learning.\n",
    "\n",
    "Find a model architecture somewhere and replicate it with code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50dc4ec",
   "metadata": {},
   "source": [
    "### 7.1 Stepping through `nn.Conv2d()`\n",
    "\n",
    "We could start using our model above and see what happens but let's first step through the two new layers we've added:\n",
    "\n",
    "nn.Conv2d(), also known as a convolutional layer.\n",
    "nn.MaxPool2d(), also known as a max pooling layer.\n",
    "\n",
    "**Question: What does the \"2d\" in nn.Conv2d() stand for?**\n",
    "\n",
    "- The 2d is for 2-dimensional data. As in, our images have two dimensions: height and width. Yes, there's color channel dimension but each of the color channel dimensions have two dimensions too: height and width.\n",
    "\n",
    "- For other dimensional data (such as 1D for text or 3D for 3D objects) there's also nn.Conv1d() and nn.Conv3d().\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4c26d6",
   "metadata": {},
   "source": [
    "To test the layers out, let's create some toy data just like the data used on CNN Explainer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "304dffe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([32, 3, 64, 64]) -> [batch_size, color_channels, height, width]\n",
      "Single Image shape: torch.Size([3, 64, 64]) -> [color_channels, height, width]\n",
      "Single Image pixel values: \n",
      "tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  1.8446, -1.1845,  1.3835],\n",
      "         [ 1.4451,  0.8564,  2.2181,  ...,  0.3399,  0.7200,  0.4114],\n",
      "         [ 1.9312,  1.0119, -1.4364,  ..., -0.5558,  0.7043,  0.7099],\n",
      "         ...,\n",
      "         [-0.5610, -0.4830,  0.4770,  ..., -0.2713, -0.9537, -0.6737],\n",
      "         [ 0.3076, -0.1277,  0.0366,  ..., -2.0060,  0.2824, -0.8111],\n",
      "         [-1.5486,  0.0485, -0.7712,  ..., -0.1403,  0.9416, -0.0118]],\n",
      "\n",
      "        [[-0.5197,  1.8524,  1.8365,  ...,  0.8935, -1.5114, -0.8515],\n",
      "         [ 2.0818,  1.0677, -1.4277,  ...,  1.6612, -2.6223, -0.4319],\n",
      "         [-0.1010, -0.4388, -1.9775,  ...,  0.2106,  0.2536, -0.7318],\n",
      "         ...,\n",
      "         [ 0.2779,  0.7342, -0.3736,  ..., -0.4601,  0.1815,  0.1850],\n",
      "         [ 0.7205, -0.2833,  0.0937,  ..., -0.1002, -2.3609,  2.2465],\n",
      "         [-1.3242, -0.1973,  0.2920,  ...,  0.5409,  0.6940,  1.8563]],\n",
      "\n",
      "        [[-0.7978,  1.0261,  1.1465,  ...,  1.2134,  0.9354, -0.0780],\n",
      "         [-1.4647, -1.9571,  0.1017,  ..., -1.9986, -0.7409,  0.7011],\n",
      "         [-1.3938,  0.8466, -1.7191,  ..., -1.1867,  0.1320,  0.3407],\n",
      "         ...,\n",
      "         [ 0.8206, -0.3745,  1.2499,  ..., -0.0676,  0.0385,  0.6335],\n",
      "         [-0.5589, -0.3393,  0.2347,  ...,  2.1181,  2.4569,  1.3083],\n",
      "         [-0.4092,  1.5199,  0.2401,  ..., -0.2558,  0.7870,  0.9924]]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a sample batch of random numbers with same size as image batch\n",
    "images = torch.randn(size=(32,3,64,64))     # [batch size, color_channels, height, width]\n",
    "test_image = images[0]      # get a single image for testing\n",
    "\n",
    "print(f\"Image batch shape: {images.shape} -> [batch_size, color_channels, height, width]\")\n",
    "print(f\"Single Image shape: {test_image.shape} -> [color_channels, height, width]\")\n",
    "print(f\"Single Image pixel values: \\n{test_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad3a9b8",
   "metadata": {},
   "source": [
    "![Convolution Explaination](../assets/03-conv2d-layer.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee351638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.5396,  0.0516,  0.6454,  ..., -0.3673,  0.8711,  0.4256],\n",
      "         [ 0.3662,  1.0114, -0.5997,  ...,  0.8983,  0.2809, -0.2741],\n",
      "         [ 1.2664, -1.4054,  0.3727,  ..., -0.3409,  1.2191, -0.0463],\n",
      "         ...,\n",
      "         [-0.1541,  0.5132, -0.3624,  ..., -0.2360, -0.4609, -0.0035],\n",
      "         [ 0.2981, -0.2432,  1.5012,  ..., -0.6289, -0.7283, -0.5767],\n",
      "         [-0.0386, -0.0781, -0.0388,  ...,  0.2842,  0.4228, -0.1802]],\n",
      "\n",
      "        [[-0.2840, -0.0319, -0.4455,  ..., -0.7956,  1.5599, -1.2449],\n",
      "         [ 0.2753, -0.1262, -0.6541,  ..., -0.2211,  0.1999, -0.8856],\n",
      "         [-0.5404, -1.5489,  0.0249,  ..., -0.5932, -1.0913, -0.3849],\n",
      "         ...,\n",
      "         [ 0.3870, -0.4064, -0.8236,  ...,  0.1734, -0.4330, -0.4951],\n",
      "         [-0.1984, -0.6386,  1.0263,  ..., -0.9401, -0.0585, -0.7833],\n",
      "         [-0.6306, -0.2052, -0.3694,  ..., -1.3248,  0.2456, -0.7134]],\n",
      "\n",
      "        [[ 0.4414,  0.5100,  0.4846,  ..., -0.8484,  0.2638,  1.1258],\n",
      "         [ 0.8117,  0.3191, -0.0157,  ...,  1.2686,  0.2319,  0.5003],\n",
      "         [ 0.3212,  0.0485, -0.2581,  ...,  0.2258,  0.2587, -0.8804],\n",
      "         ...,\n",
      "         [-0.1144, -0.1869,  0.0160,  ..., -0.8346,  0.0974,  0.8421],\n",
      "         [ 0.2941,  0.4417,  0.5866,  ..., -0.1224,  0.4814, -0.4799],\n",
      "         [ 0.6059, -0.0415, -0.2028,  ...,  0.1170,  0.2521, -0.4372]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2560, -0.0477,  0.6380,  ...,  0.6436,  0.7553, -0.7055],\n",
      "         [ 1.5595, -0.2209, -0.9486,  ..., -0.4876,  0.7754,  0.0750],\n",
      "         [-0.0797,  0.2471,  1.1300,  ...,  0.1505,  0.2354,  0.9576],\n",
      "         ...,\n",
      "         [ 1.1065,  0.6839,  1.2183,  ...,  0.3015, -0.1910, -0.1902],\n",
      "         [-0.3486, -0.7173, -0.3582,  ...,  0.4917,  0.7219,  0.1513],\n",
      "         [ 0.0119,  0.1017,  0.7839,  ..., -0.3752, -0.8127, -0.1257]],\n",
      "\n",
      "        [[ 0.3841,  1.1322,  0.1620,  ...,  0.7010,  0.0109,  0.6058],\n",
      "         [ 0.1664,  0.1873,  1.5924,  ...,  0.3733,  0.9096, -0.5399],\n",
      "         [ 0.4094, -0.0861, -0.7935,  ..., -0.1285, -0.9932, -0.3013],\n",
      "         ...,\n",
      "         [ 0.2688, -0.5630, -1.1902,  ...,  0.4493,  0.5404, -0.0103],\n",
      "         [ 0.0535,  0.4411,  0.5313,  ...,  0.0148, -1.0056,  0.3759],\n",
      "         [ 0.3031, -0.1590, -0.1316,  ..., -0.5384, -0.4271, -0.4876]],\n",
      "\n",
      "        [[-1.1865, -0.7280, -1.2331,  ..., -0.9013, -0.0542, -1.5949],\n",
      "         [-0.6345, -0.5920,  0.5326,  ..., -1.0395, -0.7963, -0.0647],\n",
      "         [-0.1132,  0.5166,  0.2569,  ...,  0.5595, -1.6881,  0.9485],\n",
      "         ...,\n",
      "         [-0.0254, -0.2669,  0.1927,  ..., -0.2917,  0.1088, -0.4807],\n",
      "         [-0.2609, -0.2328,  0.1404,  ..., -0.1325, -0.8436, -0.7524],\n",
      "         [-1.1399, -0.1751, -0.8705,  ...,  0.1589,  0.3377,  0.3493]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "torch.Size([3, 64, 64])\n",
      "torch.Size([10, 62, 62])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a convolutional layer with the same dimensinos as TinyVGG\n",
    "conv_layer = nn.Conv2d(in_channels = 3,\n",
    "                       out_channels = 10,\n",
    "                       kernel_size = 3,\n",
    "                       stride = 1,\n",
    "                       padding = 0)\n",
    "\n",
    "result = conv_layer(test_image)\n",
    "print(result)\n",
    "print(test_image.shape)\n",
    "print(result.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101b5979",
   "metadata": {},
   "source": [
    "If we try to pass a single image in, we get a shape mismatch error:\n",
    "\n",
    "RuntimeError: Expected 4-dimensional input for 4-dimensional weight [10, 3, 3, 3], but got 3-dimensional input of size [3, 64, 64] instead\n",
    "\n",
    "**Note:** If you're running PyTorch 1.11.0+, this error won't occur.\n",
    "\n",
    "This is because our nn.Conv2d() layer expects a 4-dimensional tensor as input with size (N, C, H, W) or [batch_size, color_channels, height, width].\n",
    "\n",
    "Right now our single image test_image only has a shape of [color_channels, height, width] or [3, 64, 64].\n",
    "\n",
    "We can fix this for a single image using test_image.unsqueeze(dim=0) to add an extra dimension for N.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98be64f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image.unsqueeze(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9fc8be11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 62, 62])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass the test image with extra dimension through conv_layer\n",
    "conv_layer(test_image.unsqueeze(dim=0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce0650a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 30, 30])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a new conv_layer with different values (try setting these to whatever you like)\n",
    "conv_layer_2 = nn.Conv2d(in_channels = 3,\n",
    "                         out_channels = 10,\n",
    "                         kernel_size = (5,5),   # kernel is usually a square so a tuple also works\n",
    "                         stride = 2,\n",
    "                         padding = 0)\n",
    "\n",
    "# Pass single image through new conv_layer_2 (this calls nn.Conv2d()'s forward() method on the input)\n",
    "conv_layer_2(test_image.unsqueeze(dim=0)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a777291",
   "metadata": {},
   "source": [
    "**What's going on here?**\n",
    "\n",
    "Behind the scenes, our nn.Conv2d() is compressing the information stored in the image.\n",
    "\n",
    "It does this by performing operations on the input (our test image) against its internal parameters.\n",
    "\n",
    "The goal of this is similar to all of the other neural networks we've been building.\n",
    "\n",
    "Data goes in and the layers try to update their internal parameters (patterns) to lower the loss function thanks to some help of the optimizer.\n",
    "\n",
    "The only difference is how the different layers calculate their parameter updates or in PyTorch terms, the operation present in the layer forward() method.\n",
    "\n",
    "If we check out our conv_layer_2.state_dict() we'll find a similar weight and bias setup as we've seen before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "620aea77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[[[ 0.0883,  0.0958, -0.0271,  0.1061, -0.0253],\n",
      "          [ 0.0233, -0.0562,  0.0678,  0.1018, -0.0847],\n",
      "          [ 0.1004,  0.0216,  0.0853,  0.0156,  0.0557],\n",
      "          [-0.0163,  0.0890,  0.0171, -0.0539,  0.0294],\n",
      "          [-0.0532, -0.0135, -0.0469,  0.0766, -0.0911]],\n",
      "\n",
      "         [[-0.0532, -0.0326, -0.0694,  0.0109, -0.1140],\n",
      "          [ 0.1043, -0.0981,  0.0891,  0.0192, -0.0375],\n",
      "          [ 0.0714,  0.0180,  0.0933,  0.0126, -0.0364],\n",
      "          [ 0.0310, -0.0313,  0.0486,  0.1031,  0.0667],\n",
      "          [-0.0505,  0.0667,  0.0207,  0.0586, -0.0704]],\n",
      "\n",
      "         [[-0.1143, -0.0446, -0.0886,  0.0947,  0.0333],\n",
      "          [ 0.0478,  0.0365, -0.0020,  0.0904, -0.0820],\n",
      "          [ 0.0073, -0.0788,  0.0356, -0.0398,  0.0354],\n",
      "          [-0.0241,  0.0958, -0.0684, -0.0689, -0.0689],\n",
      "          [ 0.1039,  0.0385,  0.1111, -0.0953, -0.1145]]],\n",
      "\n",
      "\n",
      "        [[[-0.0903, -0.0777,  0.0468,  0.0413,  0.0959],\n",
      "          [-0.0596, -0.0787,  0.0613, -0.0467,  0.0701],\n",
      "          [-0.0274,  0.0661, -0.0897, -0.0583,  0.0352],\n",
      "          [ 0.0244, -0.0294,  0.0688,  0.0785, -0.0837],\n",
      "          [-0.0616,  0.1057, -0.0390, -0.0409, -0.1117]],\n",
      "\n",
      "         [[-0.0661,  0.0288, -0.0152, -0.0838,  0.0027],\n",
      "          [-0.0789, -0.0980, -0.0636, -0.1011, -0.0735],\n",
      "          [ 0.1154,  0.0218,  0.0356, -0.1077, -0.0758],\n",
      "          [-0.0384,  0.0181, -0.1016, -0.0498, -0.0691],\n",
      "          [ 0.0003, -0.0430, -0.0080, -0.0782, -0.0793]],\n",
      "\n",
      "         [[-0.0674, -0.0395, -0.0911,  0.0968, -0.0229],\n",
      "          [ 0.0994,  0.0360, -0.0978,  0.0799, -0.0318],\n",
      "          [-0.0443, -0.0958, -0.1148,  0.0330, -0.0252],\n",
      "          [ 0.0450, -0.0948,  0.0857, -0.0848, -0.0199],\n",
      "          [ 0.0241,  0.0596,  0.0932,  0.1052, -0.0916]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0291, -0.0497, -0.0127, -0.0864,  0.1052],\n",
      "          [-0.0847,  0.0617,  0.0406,  0.0375, -0.0624],\n",
      "          [ 0.1050,  0.0254,  0.0149, -0.1018,  0.0485],\n",
      "          [-0.0173, -0.0529,  0.0992,  0.0257, -0.0639],\n",
      "          [-0.0584, -0.0055,  0.0645, -0.0295, -0.0659]],\n",
      "\n",
      "         [[-0.0395, -0.0863,  0.0412,  0.0894, -0.1087],\n",
      "          [ 0.0268,  0.0597,  0.0209, -0.0411,  0.0603],\n",
      "          [ 0.0607,  0.0432, -0.0203, -0.0306,  0.0124],\n",
      "          [-0.0204, -0.0344,  0.0738,  0.0992, -0.0114],\n",
      "          [-0.0259,  0.0017, -0.0069,  0.0278,  0.0324]],\n",
      "\n",
      "         [[-0.1049, -0.0426,  0.0972,  0.0450, -0.0057],\n",
      "          [-0.0696, -0.0706, -0.1034, -0.0376,  0.0390],\n",
      "          [ 0.0736,  0.0533, -0.1021, -0.0694, -0.0182],\n",
      "          [ 0.1117,  0.0167, -0.0299,  0.0478, -0.0440],\n",
      "          [-0.0747,  0.0843, -0.0525, -0.0231, -0.1149]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0773,  0.0875,  0.0421, -0.0805, -0.1140],\n",
      "          [-0.0938,  0.0861,  0.0554,  0.0972,  0.0605],\n",
      "          [ 0.0292, -0.0011, -0.0878, -0.0989, -0.1080],\n",
      "          [ 0.0473, -0.0567, -0.0232, -0.0665, -0.0210],\n",
      "          [-0.0813, -0.0754,  0.0383, -0.0343,  0.0713]],\n",
      "\n",
      "         [[-0.0370, -0.0847, -0.0204, -0.0560, -0.0353],\n",
      "          [-0.1099,  0.0646, -0.0804,  0.0580,  0.0524],\n",
      "          [ 0.0825, -0.0886,  0.0830, -0.0546,  0.0428],\n",
      "          [ 0.1084, -0.0163, -0.0009, -0.0266, -0.0964],\n",
      "          [ 0.0554, -0.1146,  0.0717,  0.0864,  0.1092]],\n",
      "\n",
      "         [[-0.0272, -0.0949,  0.0260,  0.0638, -0.1149],\n",
      "          [-0.0262, -0.0692, -0.0101, -0.0568, -0.0472],\n",
      "          [-0.0367, -0.1097,  0.0947,  0.0968, -0.0181],\n",
      "          [-0.0131, -0.0471, -0.1043, -0.1124,  0.0429],\n",
      "          [-0.0634, -0.0742, -0.0090, -0.0385, -0.0374]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0037, -0.0245, -0.0398, -0.0553, -0.0940],\n",
      "          [ 0.0968, -0.0462,  0.0306, -0.0401,  0.0094],\n",
      "          [ 0.1077,  0.0532, -0.1001,  0.0458,  0.1096],\n",
      "          [ 0.0304,  0.0774,  0.1138, -0.0177,  0.0240],\n",
      "          [-0.0803, -0.0238,  0.0855,  0.0592, -0.0731]],\n",
      "\n",
      "         [[-0.0926, -0.0789, -0.1140, -0.0891, -0.0286],\n",
      "          [ 0.0779,  0.0193, -0.0878, -0.0926,  0.0574],\n",
      "          [-0.0859, -0.0142,  0.0554, -0.0534, -0.0126],\n",
      "          [-0.0101, -0.0273, -0.0585, -0.1029, -0.0933],\n",
      "          [-0.0618,  0.1115, -0.0558, -0.0775,  0.0280]],\n",
      "\n",
      "         [[ 0.0318,  0.0633,  0.0878,  0.0643, -0.1145],\n",
      "          [ 0.0102,  0.0699, -0.0107, -0.0680,  0.1101],\n",
      "          [-0.0432, -0.0657, -0.1041,  0.0052,  0.0512],\n",
      "          [ 0.0256,  0.0228, -0.0876, -0.1078,  0.0020],\n",
      "          [ 0.1053,  0.0666, -0.0672, -0.0150, -0.0851]]],\n",
      "\n",
      "\n",
      "        [[[-0.0557,  0.0209,  0.0629,  0.0957, -0.1060],\n",
      "          [ 0.0772, -0.0814,  0.0432,  0.0977,  0.0016],\n",
      "          [ 0.1051, -0.0984, -0.0441,  0.0673, -0.0252],\n",
      "          [-0.0236, -0.0481,  0.0796,  0.0566,  0.0370],\n",
      "          [-0.0649, -0.0937,  0.0125,  0.0342, -0.0533]],\n",
      "\n",
      "         [[-0.0323,  0.0780,  0.0092,  0.0052, -0.0284],\n",
      "          [-0.1046, -0.1086, -0.0552, -0.0587,  0.0360],\n",
      "          [-0.0336, -0.0452,  0.1101,  0.0402,  0.0823],\n",
      "          [-0.0559, -0.0472,  0.0424, -0.0769, -0.0755],\n",
      "          [-0.0056, -0.0422, -0.0866,  0.0685,  0.0929]],\n",
      "\n",
      "         [[ 0.0187, -0.0201, -0.1070, -0.0421,  0.0294],\n",
      "          [ 0.0544, -0.0146, -0.0457,  0.0643, -0.0920],\n",
      "          [ 0.0730, -0.0448,  0.0018, -0.0228,  0.0140],\n",
      "          [-0.0349,  0.0840, -0.0030,  0.0901,  0.1110],\n",
      "          [-0.0563, -0.0842,  0.0926,  0.0905, -0.0882]]],\n",
      "\n",
      "\n",
      "        [[[-0.0089, -0.1139, -0.0945,  0.0223,  0.0307],\n",
      "          [ 0.0245, -0.0314,  0.1065,  0.0165, -0.0681],\n",
      "          [-0.0065,  0.0277,  0.0404, -0.0816,  0.0433],\n",
      "          [-0.0590, -0.0959, -0.0631,  0.1114,  0.0987],\n",
      "          [ 0.1034,  0.0678,  0.0872, -0.0155, -0.0635]],\n",
      "\n",
      "         [[ 0.0577, -0.0598, -0.0779, -0.0369,  0.0242],\n",
      "          [ 0.0594, -0.0448, -0.0680,  0.0156, -0.0681],\n",
      "          [-0.0752,  0.0602, -0.0194,  0.1055,  0.1123],\n",
      "          [ 0.0345,  0.0397,  0.0266,  0.0018, -0.0084],\n",
      "          [ 0.0016,  0.0431,  0.1074, -0.0299, -0.0488]],\n",
      "\n",
      "         [[-0.0280, -0.0558,  0.0196,  0.0862,  0.0903],\n",
      "          [ 0.0530, -0.0850, -0.0620, -0.0254, -0.0213],\n",
      "          [ 0.0095, -0.1060,  0.0359, -0.0881, -0.0731],\n",
      "          [-0.0960,  0.1006, -0.1093,  0.0871, -0.0039],\n",
      "          [-0.0134,  0.0722, -0.0107,  0.0724,  0.0835]]],\n",
      "\n",
      "\n",
      "        [[[-0.1003,  0.0444,  0.0218,  0.0248,  0.0169],\n",
      "          [ 0.0316, -0.0555, -0.0148,  0.1097,  0.0776],\n",
      "          [-0.0043, -0.1086,  0.0051, -0.0786,  0.0939],\n",
      "          [-0.0701, -0.0083, -0.0256,  0.0205,  0.1087],\n",
      "          [ 0.0110,  0.0669,  0.0896,  0.0932, -0.0399]],\n",
      "\n",
      "         [[-0.0258,  0.0556, -0.0315,  0.0541, -0.0252],\n",
      "          [-0.0783,  0.0470,  0.0177,  0.0515,  0.1147],\n",
      "          [ 0.0788,  0.1095,  0.0062, -0.0993, -0.0810],\n",
      "          [-0.0717, -0.1018, -0.0579, -0.1063, -0.1065],\n",
      "          [-0.0690, -0.1138, -0.0709,  0.0440,  0.0963]],\n",
      "\n",
      "         [[-0.0343, -0.0336,  0.0617, -0.0570, -0.0546],\n",
      "          [ 0.0711, -0.1006,  0.0141,  0.1020,  0.0198],\n",
      "          [ 0.0314, -0.0672, -0.0016,  0.0063,  0.0283],\n",
      "          [ 0.0449,  0.1003, -0.0881,  0.0035, -0.0577],\n",
      "          [-0.0913, -0.0092, -0.1016,  0.0806,  0.0134]]],\n",
      "\n",
      "\n",
      "        [[[-0.0622,  0.0603, -0.1093, -0.0447, -0.0225],\n",
      "          [-0.0981, -0.0734, -0.0188,  0.0876,  0.1115],\n",
      "          [ 0.0735, -0.0689, -0.0755,  0.1008,  0.0408],\n",
      "          [ 0.0031,  0.0156, -0.0928, -0.0386,  0.1112],\n",
      "          [-0.0285, -0.0058, -0.0959, -0.0646, -0.0024]],\n",
      "\n",
      "         [[-0.0717, -0.0143,  0.0470, -0.1130,  0.0343],\n",
      "          [-0.0763, -0.0564,  0.0443,  0.0918, -0.0316],\n",
      "          [-0.0474, -0.1044, -0.0595, -0.1011, -0.0264],\n",
      "          [ 0.0236, -0.1082,  0.1008,  0.0724, -0.1130],\n",
      "          [-0.0552,  0.0377, -0.0237, -0.0126, -0.0521]],\n",
      "\n",
      "         [[ 0.0927, -0.0645,  0.0958,  0.0075,  0.0232],\n",
      "          [ 0.0901, -0.0190, -0.0657, -0.0187,  0.0937],\n",
      "          [-0.0857,  0.0262, -0.1135,  0.0605,  0.0427],\n",
      "          [ 0.0049,  0.0496,  0.0001,  0.0639, -0.0914],\n",
      "          [-0.0170,  0.0512,  0.1150,  0.0588, -0.0840]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0888, -0.0257, -0.0247, -0.1050, -0.0182],\n",
      "          [ 0.0817,  0.0161, -0.0673,  0.0355, -0.0370],\n",
      "          [ 0.1054, -0.1002, -0.0365, -0.1115, -0.0455],\n",
      "          [ 0.0364,  0.1112,  0.0194,  0.1132,  0.0226],\n",
      "          [ 0.0667,  0.0926,  0.0965, -0.0646,  0.1062]],\n",
      "\n",
      "         [[ 0.0699, -0.0540, -0.0551, -0.0969,  0.0290],\n",
      "          [-0.0936,  0.0488,  0.0365, -0.1003,  0.0315],\n",
      "          [-0.0094,  0.0527,  0.0663, -0.1148,  0.1059],\n",
      "          [ 0.0968,  0.0459, -0.1055, -0.0412, -0.0335],\n",
      "          [-0.0297,  0.0651,  0.0420,  0.0915, -0.0432]],\n",
      "\n",
      "         [[ 0.0389,  0.0411, -0.0961, -0.1120, -0.0599],\n",
      "          [ 0.0790, -0.1087, -0.1005,  0.0647,  0.0623],\n",
      "          [ 0.0950, -0.0872, -0.0845,  0.0592,  0.1004],\n",
      "          [ 0.0691,  0.0181,  0.0381,  0.1096, -0.0745],\n",
      "          [-0.0524,  0.0808, -0.0790, -0.0637,  0.0843]]]])), ('bias', tensor([ 0.0364,  0.0373, -0.0489, -0.0016,  0.1057, -0.0693,  0.0009,  0.0549,\n",
      "        -0.0797,  0.1121]))])\n"
     ]
    }
   ],
   "source": [
    "# Check out the conv_layer_2 internal parameters\n",
    "print(conv_layer_2.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7442d2ba",
   "metadata": {},
   "source": [
    "Look at that! A bunch of random numbers for a weight and bias tensor.\n",
    "\n",
    "The shapes of these are manipulated by the inputs we passed to nn.Conv2d() when we set it up.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c3f5af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_layer_2 weight shape: \n",
      "torch.Size([10, 3, 5, 5]) -> [out_channels = 10, in_channels = 3, kernel_size = (5,5) ]\n",
      "conv_layer_2 bias shape: \n",
      "torch.Size([10]) -> [out_channels = 10] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the shapes of weight and bias tensors withing conv_layer_2\n",
    "print(f\"conv_layer_2 weight shape: \\n{conv_layer_2.weight.shape} -> [out_channels = 10, in_channels = 3, kernel_size = (5,5) ]\")\n",
    "\n",
    "print(f\"conv_layer_2 bias shape: \\n{conv_layer_2.bias.shape} -> [out_channels = 10] \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039bf503",
   "metadata": {},
   "source": [
    "**Question: What should we set the parameters of our nn.Conv2d() layers?**\n",
    "\n",
    "That's a good one. But similar to many other things in machine learning, the values of these aren't set in stone (and recall, because these values are ones we can set ourselves, they're referred to as \"hyperparameters\").\n",
    "\n",
    "The best way to find out is to try out different values and see how they effect your model's performance.\n",
    "\n",
    "Or even better, find a working example on a problem similar to yours (like we've done with TinyVGG) and copy it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fec773",
   "metadata": {},
   "source": [
    "### 7.2 Stepping through `nn.MaxPool2d()`\n",
    "\n",
    "Let's check what happens when we move data through `nn.MaxPool2d()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50b37612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test image original shape: torch.Size([3, 64, 64])\n",
      "Test image with unsqueezed dimension: torch.Size([1, 3, 64, 64])\n",
      "\n",
      "Shape after going through conv_layer() :\n",
      " torch.Size([1, 10, 62, 62])\n",
      "\n",
      "Shape after going through conv_layer() and max_pool_layer():\n",
      " torch.Size([1, 10, 31, 31])\n"
     ]
    }
   ],
   "source": [
    "# print out the original image shape without and with unsqueezed dimension\n",
    "print(f\"Test image original shape: {test_image.shape}\")\n",
    "print(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(dim=0).shape}\\n\")\n",
    "\n",
    "# create a sample nn.MaxPool2d() layer\n",
    "max_pool_layer = nn.MaxPool2d(kernel_size = 2)\n",
    "\n",
    "# pass data through just the conv layer\n",
    "test_image_through_conv = conv_layer(test_image.unsqueeze(dim=0))\n",
    "print(f\"Shape after going through conv_layer() :\\n {test_image_through_conv.shape}\\n\")\n",
    "\n",
    "# Pass the data through max pool layer\n",
    "test_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv)\n",
    "print(f\"Shape after going through conv_layer() and max_pool_layer():\\n {test_image_through_conv_and_max_pool.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3f2ed82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random tensor: \n",
      "tensor([[[[0.3367, 0.1288],\n",
      "          [0.2345, 0.2303]]]])\n",
      "Random tensor shape: torch.Size([1, 1, 2, 2])\n",
      "\n",
      "Max pool tensor: \n",
      "tensor([[[[0.3367]]]]) <- this is the maximum value from random_tensor\n",
      "Max pool tensor shape: torch.Size([1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a random tensor with a similar number of dimensions to our images\n",
    "random_tensor = torch.randn(size=(1,1,2,2))\n",
    "print(f\"Random tensor: \\n{random_tensor}\")\n",
    "print(f\"Random tensor shape: {random_tensor.shape}\")\n",
    "\n",
    "# create a max pool layer\n",
    "max_pool_layer = nn.MaxPool2d(kernel_size=2)    # see what happens when you change the kernel_size value\n",
    "\n",
    "# pass the random tensor through the max pool layer\n",
    "max_pool_tensor = max_pool_layer(random_tensor)\n",
    "print(f\"\\nMax pool tensor: \\n{max_pool_tensor} <- this is the maximum value from random_tensor\")\n",
    "print(f\"Max pool tensor shape: {max_pool_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe98ef58",
   "metadata": {},
   "source": [
    "Notice the final two dimensions between `random_tensor` and `max_pool_tensor`, they go from `[2, 2]` to `[1, 1]`.\n",
    "\n",
    "In essence, they get halved.\n",
    "\n",
    "And the change would be different for different values of kernel_size for nn.MaxPool2d().\n",
    "\n",
    "Also notice the value leftover in max_pool_tensor is the maximum value from random_tensor.\n",
    "\n",
    "**What's happening here?**\n",
    "\n",
    "This is another important piece of the puzzle of neural networks.\n",
    "\n",
    "Essentially, every layer in a neural network is trying to compress data from higher dimensional space to lower dimensional space.\n",
    "\n",
    "In other words, take a lot of numbers (raw data) and learn patterns in those numbers, patterns that are predictive whilst also being smaller in size than the original values.\n",
    "\n",
    "From an artificial intelligence perspective, you could consider the whole goal of a neural network to compress information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d339f251",
   "metadata": {},
   "source": [
    "![Convolution as a Compression Layer](../assets/03-conv-net-as-compression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08d0bb0",
   "metadata": {},
   "source": [
    "This means, that from the point of view of a neural network, intelligence is compression.\n",
    "\n",
    "This is the idea of the use of a nn.MaxPool2d() layer: take the maximum value from a portion of a tensor and disregard the rest.\n",
    "\n",
    "In essence, lowering the dimensionality of a tensor whilst still retaining a (hopefully) significant portion of the information.\n",
    "\n",
    "It is the same story for a nn.Conv2d() layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad61a71",
   "metadata": {},
   "source": [
    "**Exercise**: What do you think the `nn.AvgPool2d()` layer does? Try making a random tensor like we did above and passing it through. Check the input and output shapes as well as the input and output values.\n",
    "\n",
    "\n",
    "- Most common neural network networks : [`torchvision.models`](https://docs.pytorch.org/vision/stable/models.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d7804",
   "metadata": {},
   "source": [
    "### 7.3 Setup a loss function and optimizer for `model_2`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71790a60",
   "metadata": {},
   "source": [
    "Now it's time to move forward and get to training!\n",
    "\n",
    "Let's setup a loss function and an optimizer.\n",
    "\n",
    "We'll use the functions as before, nn.CrossEntropyLoss() as the loss function (since we're working with multi-class classification data).\n",
    "\n",
    "And torch.optim.SGD() as the optimizer to optimize model_2.parameters() with a learning rate of 0.1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d24b20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params = model_2.parameters(), lr=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5735cd0e",
   "metadata": {},
   "source": [
    "### 7.4 Training and Testing model_2 using our training and test functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5859b2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334f1ee0ab454ad8b38de3638818715c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "--------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "linear(): input and weight.T shapes cannot be multiplied (32x1960 and 490x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m               \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m               \u001b[49m\u001b[43maccuracy_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maccuracy_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     test_step(data_loader \u001b[38;5;241m=\u001b[39m test_dataloader, model \u001b[38;5;241m=\u001b[39m model_2, \n\u001b[1;32m     15\u001b[0m               loss_fn \u001b[38;5;241m=\u001b[39m loss_fn, accuracy_fn \u001b[38;5;241m=\u001b[39m accuracy_fn,\n\u001b[1;32m     16\u001b[0m               device \u001b[38;5;241m=\u001b[39m device)\n\u001b[1;32m     18\u001b[0m train_time_end_model_2 \u001b[38;5;241m=\u001b[39m timer()\n",
      "Cell \u001b[0;32mIn[24], line 13\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, data_loader, loss_fn, optimizer, accuracy_fn, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m X,y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 1. Forward Pass\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 2. Calculate loss\u001b[39;00m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y)\n",
      "File \u001b[0;32m~/Desktop/coding-projects/pytorch_tutorials_from_official_docs/venv_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/coding-projects/pytorch_tutorials_from_official_docs/venv_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[30], line 44\u001b[0m, in \u001b[0;36mFashionMNISTModelV2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_1(x)\n\u001b[1;32m     43\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_2(x)\n\u001b[0;32m---> 44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Desktop/coding-projects/pytorch_tutorials_from_official_docs/venv_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/coding-projects/pytorch_tutorials_from_official_docs/venv_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/coding-projects/pytorch_tutorials_from_official_docs/venv_2/lib/python3.10/site-packages/torch/nn/modules/container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/coding-projects/pytorch_tutorials_from_official_docs/venv_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/coding-projects/pytorch_tutorials_from_official_docs/venv_2/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/coding-projects/pytorch_tutorials_from_official_docs/venv_2/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: linear(): input and weight.T shapes cannot be multiplied (32x1960 and 490x10)"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Measure time\n",
    "from timeit import default_timer as timer\n",
    "train_time_start_model_2 = timer()\n",
    "\n",
    "# Train and test model\n",
    "epochs = 3\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n--------------------\")\n",
    "    train_step(data_loader = train_dataloader, model=model_2,\n",
    "               loss_fn = loss_fn, optimizer = optimizer,\n",
    "               accuracy_fn = accuracy_fn, device = device)\n",
    "    test_step(data_loader = test_dataloader, model = model_2, \n",
    "              loss_fn = loss_fn, accuracy_fn = accuracy_fn,\n",
    "              device = device)\n",
    "\n",
    "train_time_end_model_2 = timer()\n",
    "total_train_time_model_2 = print_train_time(start = train_time_start_model_2,\n",
    "                                            end = train_time_end_model_2,\n",
    "                                            device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63421c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
