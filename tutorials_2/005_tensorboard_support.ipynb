{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4def8bb",
   "metadata": {},
   "source": [
    "# Tensorboard support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a47a1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Using cached tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: matplotlib in /Users/qbit-glitch/Desktop/coding-projects/pytorch_tutorials_from_official_docs/venv1/lib/python3.13/site-packages (3.10.6)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.75.1-cp313-cp313-macosx_11_0_universal2.whl.metadata (3.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Using cached markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/qbit-glitch/Desktop/coding-projects/pytorch_tutorials_from_official_docs/venv1/lib/python3.13/site-packages (from tensorboard) (2.3.3)\n",
      "Requirement already satisfied: packaging in /Users/qbit-glitch/Desktop/coding-projects/pytorch_tutorials_from_official_docs/venv1/lib/python3.13/site-packages (from tensorboard) (25.0)\n",
      "Requirement already satisfied: pillow in /Users/qbit-glitch/Desktop/coding-projects/pytorch_tutorials_from_official_docs/venv1/lib/python3.13/site-packages (from tensorboard) (11.3.0)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard)\n",
      "  Using cached protobuf-6.32.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/qbit-glitch/Desktop/coding-projects/pytorch_tutorials_from_official_docs/venv1/lib/python3.13/site-packages (from tensorboard) (80.9.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/qbit-glitch/Desktop/coding-projects/pytorch_tutorials_from_official_docs/venv1/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/qbit-glitch/Desktop/coding-projects/pytorch_tutorials_from_official_docs/venv1/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/qbit-glitch/Desktop/coding-projects/pytorch_tutorials_from_official_docs/venv1/lib/python3.13/site-packages (from matplotlib) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/qbit-glitch/Desktop/coding-projects/pytorch_tutorials_from_official_docs/venv1/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/qbit-glitch/Desktop/coding-projects/pytorch_tutorials_from_official_docs/venv1/lib/python3.13/site-packages (from matplotlib) (3.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/qbit-glitch/Desktop/coding-projects/pytorch_tutorials_from_official_docs/venv1/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /Users/qbit-glitch/Desktop/coding-projects/pytorch_tutorials_from_official_docs/venv1/lib/python3.13/site-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/qbit-glitch/Desktop/coding-projects/pytorch_tutorials_from_official_docs/venv1/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/qbit-glitch/Desktop/coding-projects/pytorch_tutorials_from_official_docs/venv1/lib/python3.13/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Using cached tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading grpcio-1.75.1-cp313-cp313-macosx_11_0_universal2.whl (11.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m533.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached markdown-3.9-py3-none-any.whl (107 kB)\n",
      "Using cached protobuf-6.32.1-cp39-abi3-macosx_10_9_universal2.whl (426 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Installing collected packages: werkzeug, tensorboard-data-server, protobuf, markdown, grpcio, absl-py, tensorboard\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [tensorboard]\u001b[0m [tensorboard]\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.3.1 grpcio-1.75.1 markdown-3.9 protobuf-6.32.1 tensorboard-2.20.0 tensorboard-data-server-0.7.2 werkzeug-3.1.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorboard matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3627b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b6554c",
   "metadata": {},
   "source": [
    "## Showing images in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdb6445",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "# Store separate training nd validation splits in ./data\n",
    "training_set = torchvision.datasets.FashionMNIST('./data',download=True, train=True, transform=transform)\n",
    "\n",
    "validation_set = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True, train=False, transform=transform)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(\n",
    "    training_set, batch_size=4, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    validation_set, batch_size=4, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "# Class Labels\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channels=False):\n",
    "    if one_channel :\n",
    "        img = img.mean(dim=0)\n",
    "    img = img/2 + 0.5\n",
    "    nping = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap='Greys')\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "\n",
    "# Extract a batch of 4 images\n",
    "dataiter = iter(training_laoder)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a97a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/fashion_mnist_experiment_1')\n",
    "\n",
    "writer.add_image('Four Fashion_MNIST Images', img_grid)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aee25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defbbcd1",
   "metadata": {},
   "source": [
    "## Graphing Scalers to Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca41b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,6,5)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b4a721",
   "metadata": {},
   "source": [
    "Now let's train a single epoch and evaluate the training vs validation set losses every 1000 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2f0dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(validation_loader))\n",
    "\n",
    "for epoch in range(1):      # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(training_loader, 0):\n",
    "        # basic training loop\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        output = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999 :        # every 1000 mini-batches ...\n",
    "            print('Batch {}'.format(i+1))\n",
    "            # check against the validation set\n",
    "            running_vloss = 0.0\n",
    "\n",
    "            # In evaluation mode, some model specific operations can be omitted eg. dropout layer\n",
    "            net.train(False)    # switching to evaluation mode, eg. turning off regularisation\n",
    "            for j, vdata in enumerate(validation_loader, 0):\n",
    "                vinputs, vlabels = vdata\n",
    "                voutputs = net(vinputs)\n",
    "                vloss = criterion(voutputs, vlabels)\n",
    "                running_vloss += vloss.item()\n",
    "            net.train(True)     # switching back to training mode, eg. turning on regularisation\n",
    "            avg_loss = running_loss / 1000\n",
    "            avg_vloss = running_vloss / len(validation_loader)\n",
    "\n",
    "            # Log the running loss averaged per batch\n",
    "            writer.add_scalars('Training vs Validation Loss', {'Training': avg_loss, 'Validation: ', avg_vloss}, epoch*len(training_loader) + i)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e85a01",
   "metadata": {},
   "source": [
    "## Visualizing your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8894a5b5",
   "metadata": {},
   "source": [
    "TensorBoard can also be used to examine the data flow within your model. To do this, call the add_graph() method with a model and sample input.\n",
    "\n",
    "When you switch over to TensorBoard, you should see a GRAPHS tab. Double-click the “NET” node to see the layers and data flow within your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b4026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab a single mini batch\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# add_graph will trace the sample input through your model and render it as a graph\n",
    "writer.add_graph(net, images)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18239da9",
   "metadata": {},
   "source": [
    "## Visualize your Dataset with Embeddings\n",
    "\n",
    "The `add_embedding()` method will project a set of data onto the three dimensions with highest variance, and display them as an interactive 3D chart. The add_embedding() method does this automatically by projecting to the three dimensions with highest variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cb9ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random subset of data and corresponding labels\n",
    "def select_n_random(data, labels, n=100):\n",
    "    assert len(data) == len(labels)\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "# Extract a random subsets of data\n",
    "images, labels = select_n_random(training_set.data, training_set.targets)\n",
    "\n",
    "# get the class labels for each image\n",
    "class_labels = [classes[label] for label in labels]\n",
    "\n",
    "features = images.view(-1, 28*28)\n",
    "writer.add_embedding(features,\n",
    "    metadat=class_labels, label_img=images.unsqueeze(1))\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
